{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pengenalan Pemrosesan Bahasa Alami\n",
    "\n",
    "Oleh: Ali Akbar Septiandri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memuat Dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'soc.religion.christian',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.windows.x',\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey',\n",
    "]\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = newsgroups_train['data'], newsgroups_train['target']\n",
    "X_test, y_test = newsgroups_test['data'], newsgroups_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing, Stemming, Lemmatizing\n",
    "\n",
    "Umumnya, kata \"token\" merujuk pada satuan kata. Namun, tidak jarang kita perlu memecah paragraf menjadi kalimat sehingga kita memerlukan *sentence tokenizer*. Contoh penggunaannya dengan NLTK adalah sebagai berikut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.sent_tokenize(X_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.sent_tokenize(X_train[3])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Tokenizer\n",
    "\n",
    "Untuk berbagai bahasa (termasuk bahasa Indonesia), salah satu tokenizer terbaik yang dapat digunakan adalah dari spaCy. Dengan satu kali masukan, Anda dapat menghasilkan token yang langsung mempunyai atribut seperti dijelaskan di [dokumentasinya](https://spacy.io/api/token#attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "[token for token in nlp(sentence) if not token.is_space]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bahwa tanda baca juga dihitung sebagai token!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer\n",
    "\n",
    "Dengan menggunakan kode yang sama, Anda hanya perlu mengganti bagian `print(token)` menjadi `print(token.lemma_)` untuk melihat bentuk kamus dari tiap token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[token.lemma_ for token in nlp(sentence) if not token.is_space]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmer\n",
    "\n",
    "Stemmer berfungsi untuk memotong imbuhan. Di pengolahan teks modern, prapemrosesan ini jarang dilakukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "tokens = ['regarding', 'programming', 'denied', 'flew']\n",
    "[stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append([token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop])\n",
    "pd.DataFrame(rows, columns=['text','lemma','pos','tag','dep','shape','alpha','stopword'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasifikasi Dokumen\n",
    "\n",
    "Pertama, kita sebaiknya melihat sebaran dari kategori dokumen pada data latih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=y_train)\n",
    "plt.yticks(range(6), newsgroups_train['target_names']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datanya terlihat tersebar cukup merata, kecuali untuk topik `alt.atheism`. Namun, kita tidak perlu menyeimbangkan kelasnya untuk saat ini dan kita dapat menggunakan akurasi sebagai metrik evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil transformasi bag-of-words dari teks menghasilkan *sparse matrix*. Informasi di tiap sel dalam matriks tersebut adalah jumlah kemunculan suatu kata dalam suatu dokumen. Sekarang, kita akan mencoba melakukan klasifikasi datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(\n",
    "    CountVectorizer(),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=300, multi_class='auto', random_state=42)\n",
    ")\n",
    "acc = cross_val_score(clf, X_train, y_train, cv=3, n_jobs=2)\n",
    "print('Akurasi: {:.2%} ± {:.2%}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apa kelemahan dari model ini? Beberapa hal yang dapat dicoba:\n",
    "\n",
    "1. Bag-of-words $\\rightarrow$ n-gram\n",
    "2. Bag-of-words $\\rightarrow$ TF-IDF\n",
    "3. Buang stopwords\n",
    "4. Tokenizer $\\rightarrow$ Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    \"\"\"Mengembalikan list of lemmas\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kode Anda di sini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduksi Dimensi\n",
    "\n",
    "Salah satu cara mudah untuk mengevaluasi data sebelum melakukan pemodelan adalah dengan visualisasi. Namun, dengan dimensi yang begitu besar, bagaimana cara memvisualisasikannya sementara manusia hanya baik mengolah gambar dalam dua dimensi?\n",
    "\n",
    "Untuk kebutuhan tersebut, Anda dapat menggunakan metode reduksi dimensi seperti *Principal Component Analysis* (PCA). Dalam dunia pengolahan teks, menerapkan PCA pada matriks hasil n-gram dikenal juga dengan nama *Latent Semantic Analysis* (LSA). Berhubung matriks yang digunakan jarang, alternatif PCA yang digunakan adalah [Truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    TruncatedSVD(2, random_state=42)\n",
    ")\n",
    "X_map = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang, kita sudah dapat memvisualisasikan datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "for i, label in enumerate(newsgroups_train['target_names']):\n",
    "    ax.scatter(*X_map[y_train == i].T, marker='.', label=label)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bahwa dokumen-dokumen `rec.sport` berdekatan, begitu juga dokumen-dokumen di bawah kategori `comp`. Sudah sesuai dengan intuisi kita bukan? Pertanyaannya, apakah modelnya juga akan lebih baik dengan LSA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english'),\n",
    "    TruncatedSVD(100, random_state=42),\n",
    "    LogisticRegression(solver='lbfgs', max_iter=300, multi_class='auto', random_state=42)\n",
    ")\n",
    "acc = cross_val_score(clf, X_train, y_train, cv=3, n_jobs=2)\n",
    "print('Akurasi: {:.2%} ± {:.2%}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for doc in X_train:\n",
    "    sentences.append([token for token in nlp(doc) if not token.is_space])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp('cat dog banana'):\n",
    "    print(token.vector.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
